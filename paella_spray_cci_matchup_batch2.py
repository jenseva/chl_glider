# spray_cci_matchup.py
# JPS, Jan 09, 2025
# Runs in py39

# Borrowing code from FRSQ missions netcdf processing
# Using matchup code that was in a python notebook that JPS provided to Ana in 2024
# Requires connection to sushi to lookup the project for the mission

import pandas as pd
import numpy as np
import os
import warnings
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter
import matplotlib.pyplot as plt
import xarray as xr
import requests
import io
import lookup_project
import time

from urllib.parse import quote
import datetime
from collections import OrderedDict
from geopy.distance import geodesic
from loadmat import loadmat, get_operator_history
from zoneinfo import ZoneInfo

import logging
import logging.handlers as handlers

def is_unique(s):
    a = s.to_numpy() # s.values (pandas<0.24)
    return (a[0] == a).all()


logger = logging.getLogger('spray_cci_matchup') # Naming this logger 
logger.setLevel(logging.INFO)

# Console Display
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s %(name)s.%(funcName)s +%(lineno)s: %(levelname)-8s %(message)s')
ch.setFormatter(formatter)
logger.addHandler(ch)

# Load glider data from mat files on Jenns laptop:
mat_dir = './input_mat'

mat_dir = '/Volumes/spray/data/mat'
out_dir = './matchups/batch_2b/'

# 2025, Feb 04 JPS
# List of missions that did not process when batch2 was run. Generated by batch2_assessment.
# Tried first two and they reran fine. so trying will them all. 
# It could be that the mount to sushi was sometime slow to respond that it was a file or something?
list_to_process = ['16806301','23705801', '13704101', '16206301', '21306401', '19702501', '16B01101', '22403501', '22A02001', '21705501', '19803001', '15702801', '14602501', '20503001', '13B05801', '17805501', '20106401', '14B02501', '11602501', '11A02801', '10204101', '09201301', '12104701', '11501301', '19806801', '11101101', '10602801', '06A01301', '07101201', '18104001', '09502801', '22B06401', '09801301', '22303001', '13B04101', '18402501', '14602801', '21C02501', '10402501', '06901101', '08301301', '23202801', '12404101', '11A01301', '08101101', '22A01101', '06200501', '15A06401', '12805201', '19105701', '11603001', '22805501', '08703001', '23902801', '15104801', '22702501', '09303901', '10A01401', '14B05101', '23701301', '07701101', '22B05801', '12804801', '11501401', '15303501', '16506401', '19405801', '18804001', '11200501', '21805801', '16203001', '17302501', '17B06401', '12803001', '15103001', '09C02801', '08402801', '16601101', '18805801', '17605801', '08902501', '18503001', '18904101', '17705701', '16802801', '12501301', '13705801', '19304001', '10403001', '21A02801', '11104101', '21404101', '07501201', '20602801', '15404001', '08B01401', '13A06001', '14305701', '19205501', '21204001', '12801401', '17305501', '15B02801', '10704101', '09403001', '20806801', '20904101', '11C02501', '15B01101', '23203001', '10304901']

# FFFFFFFFFFFFFFFFFFFFF
# Looks through the whole directory of available mat files for the one(s) to process
# FFFFFFFFFFFFFFFFFFFFF

mat_count = 0
print(len(os.listdir(mat_dir)))
mat_total = len(os.listdir(mat_dir))

for in_fn in os.listdir(mat_dir):

    mat_count = mat_count + 1
    
    f = os.path.join(mat_dir, in_fn)
    
    mission_name = in_fn[:-4]

    print(mission_name)
    skip = 0

    #For list to process, filter out before starting the log file
    if [s for s in list_to_process if mission_name in s]:
        print('found it') 
    else:
        continue

    # This worked when list was length one, when longer was not workinig as expected
    #for mission in list_to_process:
    #    if mission not in f:
         #   logger.info('setting skip')
    #        skip = 1
    #        break
    #if skip ==1: 
        #logger.info('Set to skip. Skipping.')
    #    continue

    # Log File
    log_fn = './log/batch_2b/'+ mission_name +'.log'

    # Make sure old file handler is closed before starting another one.
    handler_removed = 0
    try:
        logger.removeHandler(fh)
    except KeyboardInterrupt:
        logger.info("\nCtrl+C detected. Exiting gracefully.")
    except:
        pass
    else:
        handler_removed = 1

    fh = logging.FileHandler(log_fn, mode='a') # don't switch to w without making dynamic filename
    fh.setLevel(logging.INFO)
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    logger.info('-----------------------------')
    logger.info('File ' +str(mat_count)+' of ' + str(mat_total))
    logger.info(f)
    logger.info(mission_name)

    # For debugging log file generation. Once confirmed fixed, can remove.
    #if handler_removed == 1:
    #    logger.info('previous log file handler has been removed.')

    # Confirm it is a file
    if os.path.isfile(f):
        pass
    else:
        logger.info(' Skipping, not a file')
        continue

    #skip = 0
    # Check to see if in list to skip
    #for mission in list_to_skip:
    #    if mission in f:
    #        logger.info('setting skip')
    #        skip = 1
    #        break
    #if skip ==1: 
    #    logger.info('In list to skip. Skipping.')
    #    continue

    #Check to see if mission already processed
    already_processed = 0
    for matchup_fn in os.listdir(out_dir):
        if matchup_fn[:8] == mission_name:
            already_processed = 1
            #logger.info('Setting to skip. Already processed')
            break
    if already_processed == 1:
        logger.info('Skipping. Already processed')
        continue

    if len(mission_name) != 8:
        logger.info('not an 8 char filename. Skipping')
        continue


    # Calls new load mat function, JPS 2023
    # If that fails due to new file format calls alternate reader
    try:
        mraw = loadmat(f)
    except KeyboardInterrupt:
        logger.info("\nCtrl+C detected. Exiting gracefully.")


    try:
        sp_data = mraw['bindata']
    except KeyboardInterrupt:
        logger.info("\nCtrl+C detected. Exiting gracefully.")
    except:
        logger.error('Issue loading bindata. Skipping')
        continue
    #print(sp_data)
    #print(sp_data['time'])    
    try:
        g_datetime = datetime.datetime.utcfromtimestamp(np.nanmin(sp_data['time']))
        sp_time_str_start = g_datetime.strftime("%Y-%m-%dT%H:%M:%SZ")
        g_datetime = datetime.datetime.utcfromtimestamp(np.nanmax(sp_data['time']))
        sp_time_str_end = g_datetime.strftime("%Y-%m-%dT%H:%M:%SZ")
        logger.info('Spray mission time range: ' + sp_time_str_start +' to ' + sp_time_str_end)
        logger.info('Number of Profiles: ' + str(len(sp_data["time"])))
    except KeyboardInterrupt:
        logger.info("\nCtrl+C detected. Exiting gracefully.")
    except:
        logger.error('Issue reading times. Skipping')
        continue

    project_id=''
    try:
        project_id = lookup_project.lookup_project(mission_name, 'sushi')
        logger.info('Project: '+ project_id)
    except KeyboardInterrupt:
        logger.info("\nCtrl+C detected. Exiting gracefully.")
    except:
        logger.error('Skipping, Issue with project')
        continue
    else:
        if project_id == '':
            logger.info('Skipping, Empty project')
            continue
    
    if 'cugn_l' not in project_id:
        logger.info('Project not cugn_line. Skipping.')
        continue

    project_ids = project_id.split(',')
    
    for project in project_ids:
        if 'cugn_l' in project:
            cugn_project = project[6:]
            logger.info(cugn_project)
            break

    # Get mission_name, profile Num, time, lat, lon, depth, chla_val, 
    spray_cols = ['sp_mission_id', 
    'sp_project', 
    'sp_profile_num', 
    'sp_time', 
    'sp_time_str', 
    'sp_lat', 
    'sp_lon', 
    #'sp_depth',
     'sp_fl']

    # Check out the cci metadata. This was slow on the pml server. 
    #cci_url = 'https://coastwatch.pfeg.noaa.gov/erddap/griddap/pmlEsaCCI60OceanColorDaily'
    #ds = xr.open_dataset(cci_url)
    #print('Satellite Data:')
    #print('  Time Range: ', ds.attrs['time_coverage_start'], 
    #      ds.attrs['time_coverage_end'])
    #print('  Latitude Range: ', ds.attrs['geospatial_lat_min'], 
    #      ds.attrs['geospatial_lat_max'])
    #print('  Longitude Range: ', ds.attrs['geospatial_lon_min'], 
    #      ds.attrs['geospatial_lon_max'])
    #print(' ')
    #print('Glider Data:')
    #print('  Time Range: ', sp_data['time'].min(), sp_data['time'].max())
    #print('  Latitude Range: ', 
    #      round(sp_data['lat'].min(), 2), round(sp_data['lat'].max(), 2))
    #print('  Longitude Range: ', 
    #     round(sp_data['lon'].min(), 2), round(sp_data['lon'].max(), 2))

    # create variables for the unchanging parts of the ERDDAP data-request URL.   
    #base_url = 'https://coastwatch.pfeg.noaa.gov/erddap/griddap/' # too slow
    base_url='https://spraydata.ucsd.edu/erddap/griddap/'
    dataset_id = "esa_occci_v6"
    file_type = '.csv0'
    query_start = '?'
    erddap_variables = 'chlor_a,chlor_a_log10_bias,chlor_a_log10_rmsd,total_nobs_sum,MODISA_nobs_sum,OLCI_A_nobs_sum,OLCI_B_nobs_sum,SeaWiFS_nobs_sum,VIIRS_nobs_sum'.split(',')

    # Check that glider time max is not greater than cci time max
    cci_metadata_url = base_url+dataset_id
    ds = xr.open_dataset(cci_metadata_url)
    logger.info('CCIv6 Time Range: ' + str(ds.attrs['time_coverage_start']) +' to '+ str(ds.attrs['time_coverage_end']))

    origin_tz = ZoneInfo("UTC")
    cci_timemax = ds.attrs['time_coverage_end']
    cci_maxdatetime = datetime.datetime.strptime(cci_timemax, "%Y-%m-%dT%H:%M:%SZ")
    g_maxdatetime = datetime.datetime.utcfromtimestamp(np.nanmax(sp_data['time']))
    if g_maxdatetime > cci_maxdatetime:
        logger.debug(g_maxdatetime.strftime("%Y-%m-%dT%H:%M:%SZ"))
        logger.debug('Mission too new. No cci data for all or part of it. Not processing. Skipping.')
        continue            

    # create the start of the ERDDAP data-request URL by joining URL components
    start_url = ''.join([base_url,
                         dataset_id,
                         file_type,
                         query_start])

    time_thres = datetime.timedelta(days = 0.5) 
    lat_thres = 0.1
    lon_thres = 0.1 
    sat_start = datetime.datetime.now()
    sat_cols = ["cci_time", 
        "cci_lat", 
        "cci_lon", 
        "cci_chla", 
        "cci_chlor_a_log10_bias",
        "cci_chlor_a_log10_rmsd",
        "cci_total_nobs_sum",
        "cci_MODISA_nobs_sum",
        "cci_OLCI_A_nobs_sum",
        "cci_OLCI_B_nobs_sum",
        "cci_SeaWiFS_nobs_sum",
        "cci_VIIRS_nobs_sum",
        "dist_km",
        #"cci_query",
        ]

    all_cols = spray_cols + sat_cols
    df = pd.DataFrame(columns=all_cols)
    
    out_fn = out_dir+ mission_name + '_occciv6_8day_matchup.csv'
    outfile = open(out_fn, 'wb')

    # PPPPPPPPPPPPPPPPPPPPPPPPP
    # FOR EACH PROFILE/DIVE 
    # PPPPPPPPPPPPPPPPPPPPPPPPP
    for i in range(0, len(sp_data['time'])):
        #logger.info(df)
        new_sat = pd.DataFrame(columns=sat_cols)

        logger.info(mission_name +', Profile ' + str(i+1) + ' of ' + str(len(sp_data['time'])))
        
        # Use glider profile information to form the satellite data request
        # With this approach the same satellite data will be found for many of the data points
        #sp_time = sp_data['time'][i]
        #print(sp_time)
        
        # Glider point datetime
        #g_datetime = datetime.datetime.strptime(sp_data['time'][i], "%Y-%m-%dT%H:%M:%SZ")
        try:
            g_datetime = datetime.datetime.utcfromtimestamp(sp_data['time'][i])
        except:
            logger.info('Glider has nan for time. Skipping this profile.')
            continue

        sp_time_str = g_datetime.strftime("%Y-%m-%dT%H:%M:%SZ")
        logger.info(sp_time_str)

        # Datetimes for the request 
        timemin_dt = g_datetime - time_thres
        timemax_dt = g_datetime + time_thres
        
        # String format for request
        timemin = timemin_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        timemax = timemax_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        
        # Spatial bounds for the request
        latmin = str( round(sp_data['lat'][i] - lat_thres, 7))
        latmax = str(round(sp_data['lat'][i] + lat_thres, 7))
        lonmin = str(round(sp_data['lon'][i] - lon_thres, 7))
        lonmax = str(round(sp_data['lon'][i] + lon_thres, 7))
        
        query_url = ''
        for erddap_var in erddap_variables:
            var_url =  ''.join([erddap_var,
                                 '[(' + timemin + '):1:(' + timemax + ')]',
                                 '[(' + latmin + '):1:(' + latmax + ')]', 
                                 '[(' + lonmin + '):1:(' + lonmax + ')]',
                                 ','
                                 ])
            query_url = query_url + var_url

        logger.info(start_url + query_url[:-1])
        encoded_query = quote(query_url[:-1], safe='')
        url = start_url + encoded_query

        #logger.info(url)
        # Download the data as a CSV file directly into Pandas
        # csv is not saved to machine
        # The CCI Dataset is ultimately hosted on an ESA server 
        # which appears to be painfully slow over VPN. VPN may be trottled.
        
        
        try:
            new_sat = pd.read_csv(url, skiprows=0)
        except KeyboardInterrupt:
            logger.info("\nCtrl+C detected. Exiting gracefully.")
        except Exception:
            logger.info('Request Error. Continuing.')
            continue
        #logger.info(new_sat)
        # Assign column names to response
        new_sat.columns = sat_cols[:len(sat_cols)-1]
        #logger.info(new_sat)
        logger.info('Number of cells returned in search radius: ' +str(len(new_sat)))
        
        if new_sat['cci_chla'].isnull().all():
            # Return an empty row to show obs with no matches available
            logger.debug('** No valid satellite matches within search area')
            
            #new_sat['cci_query'] = url
            new_sat['sp_mission_id'] = mission_name
            new_sat['sp_project'] = cugn_project
            new_sat['sp_profile_num'] = i+1
            new_sat['sp_time'] = sp_data['time'][i]
            new_sat['sp_lat'] =  round(sp_data['lat'][i], 7)
            new_sat['sp_lon'] = round(sp_data['lon'][i], 7)
            #new_sat['sp_depth'] = sp_data['depth'][0]
            new_sat['sp_time_str'] = sp_time_str
            new_sat['sp_fl'] = round(sp_data['fl'][0][i], 7)

            # Add the the info to the full df
            df = pd.concat([df, new_sat], ignore_index=True)
            #df.to_csv(outfile, index=False)

        else:
            logger.info('* Found some data within initial constraints, finding best match...')
            
            # NAN FILTER
            # First, filter out matches without chl values, false returns
            # Reasons for no satellite chl include land and clouds
            new_sat = new_sat.loc[new_sat['cci_chla'].notnull()]
            #logger.info(new_sat)
            new_sat = new_sat.reset_index(drop=True)
            
            #logger.info(len(new_sat))
            # TIME FILTER
            # Sometimes more than one time is returned.
            # If more than one time is returned, filter to only use the closest
            # Dan may want to filter spray data by time of day 
            
            if not is_unique(new_sat['cci_time']):
                logger.debug(' More than one time returned, filtering to closest.')    
                # find closest time
                cci_datetimes=[]
                for j in range(0, len(new_sat)):
                    cci_datetimes.append(datetime.datetime.strptime(new_sat['cci_time'].iloc[j], "%Y-%m-%dT%H:%M:%SZ"))
                             
                # filter to closest timestamp
                time_diffs = [abs(g_datetime-item) for item in cci_datetimes]
                which_time_diffs = [i for i,v in enumerate(time_diffs) if v == min(time_diffs)]
                new_sat = new_sat.loc[which_time_diffs]
                new_sat = new_sat.reset_index(drop=True)

            # Calculate spatial and time distances
            glider_loc = (sp_data['lat'][i], sp_data['lon'][i])
            dists = []; time_diffs = []
            
            #logger.info(len(new_sat))
            #logger.info(len(new_sat))

            for j in range(0, len(new_sat)):
                satellite_loc = (new_sat['cci_lat'].iloc[j], new_sat['cci_lon'].iloc[j])
                
                dist = geodesic(glider_loc, satellite_loc).km
                dists.append(round(dist,7))

                # Working on this. Want two things 
                # 1. a sanity chek that the data returned are reasonable. 
                #    Erddap will return the closest regardless of how far away the data are.
                # 2. Time of day difference calculation.

                satellite_time = new_sat['cci_time'].iloc[j]
                g_datetime_utc = g_datetime.replace(tzinfo=origin_tz)
                cci_datetime = datetime.datetime.strptime(satellite_time, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=origin_tz)
                
                time_diff = abs(cci_datetime - g_datetime_utc)
                seconds = time_diff.total_seconds() 
                time_diff_hours = round(seconds / (60 * 60), 1)
                time_diffs.append(time_diff_hours)

            # If there is anything considered a matchup that is more than x days apart
            if max(time_diffs) > 24 * 8:
                logger.debug('ALERT!! Closest time match was greater than 8 days. Skipping.')
                continue
            
            #query_list=[]; 
            project_list = []; mission_list = []; profile_num_list = [];
            sp_time_list=[]; sp_time_str_list = []; sp_lat_list=[];sp_lon_list=[];
            #sp_depth_list=[]; 
            sp_fl_list=[];
            
            valid_count = 0

            #logger.info(len(new_sat))
            #logger.info(range(0, len(new_sat)))
            for k in range(0, len(new_sat)):
                #logger.info(k)
                # Check on chl returned
                #logger.info(new_sat['cci_chla'])
                #logger.info(new_sat['cci_chla'][k])
                if new_sat['cci_chla'].iloc[k] > 0:
                    valid_count=valid_count+1
                
                #query_list.append(url)
                project_list.append(cugn_project)
                mission_list.append(mission_name)
                sp_time_str_list.append(sp_time_str)

                # Tied to glider profile loop index
                profile_num_list.append(i+1)
                sp_time_list.append(round(sp_data['time'][i], 2))
                sp_lat_list.append(sp_data['lat'][i])
                sp_lon_list.append(sp_data['lon'][i])
                #sp_depth_list.append(sp_data['depth'][0])
                sp_fl_list.append(round(sp_data['fl'][0][i],7))

            try:
                logger.info('Number of cells with FL data: ' + str(valid_count))
            except:
                logger.error('logging error issue with valid count')
            new_sat['dist_km'] = dists
            #new_sat['cci_query'] = query_list
            new_sat['sp_mission_id'] = mission_list
            new_sat['sp_project'] = project_list
            new_sat['sp_profile_num'] = profile_num_list
            new_sat['sp_time'] = sp_time_list
            new_sat['sp_time_str'] = sp_time_str_list
            new_sat['sp_lat'] = sp_lat_list
            new_sat['sp_lon'] = sp_lon_list
            #new_sat['sp_depth'] = sp_depth_list
            new_sat['sp_fl'] = sp_fl_list
            new_sat['time_diff_hrs'] = time_diffs

            # Try to reduce file size by rounding the cci data
            new_sat['cci_lat'] = round(new_sat['cci_lat'],7)
            new_sat['cci_lon'] = round(new_sat['cci_lon'],7)

            # Filter to closest cell - turning off let Dan play with this.
            #new = new.loc[new['sp_dist'] == min(dists)]

            #if len(new)>1:
            #    print('ERROR: Multiple rows, only one allowed. Number of rows: ' + str(len(new)))
            #    break
            # new df should be a single row now.
            # Note: This may not be the best matchup choice, need to make plots/analyze
            logger.info(len(new_sat))
            
            logger.info('Add the matchup to the full df')
            df = pd.concat([df, new_sat], axis=0, ignore_index=True)
            #df.to_csv(outfile, index=False)  # saving each time because of failures. 
        
        #logger.info(new_sat)

        logger.info('Done with this matchup. Delaying next request.')
        # Give server a little break before the next request
        #time.sleep(0.1)

    time_to_process = datetime.datetime.now() - sat_start
    logger.info('Done with all matchups for mission: '+ mission_name +'Time to process mission: ' + str(time_to_process))
            
    df.to_csv(outfile, index=False)
    outfile.close()
    #input("Press Enter to continue...")